{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalAssignment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "CpvAziurM2md",
        "7HSoY6YY1f-o"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sRTA6gUwvq8"
      },
      "source": [
        "# Building a single model to perform Object Detection, Depth Estimation and Planes detection on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouh8ZtK39mTg"
      },
      "source": [
        "* There are three separate outputs which has three separate models. In this project, we'll integrate them into a single model:\n",
        "  * Object Detection using YoloV3\n",
        "  * Depth Estimation using MiDaS\n",
        "  * 3D Planes Detection using PlanerRCNN\n",
        "* Depth/Panes detection output size is same as input size and hences uses Encoder/Decoder architectures\n",
        "* We need a way to figure out how to merge these networks and by making use of pretrained weights, figure out a way to train them very quickly. Let's break it down into multiple steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7PxXkrJ0C-m"
      },
      "source": [
        "##Step 1: Understand the three models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoIvQP9o11pW"
      },
      "source": [
        "#### **MiDaS**\n",
        "* Uses resnext101_32x8d_wsl model with pretrained weights from PyTorch Hub: `facebookresearch/WSL-Images`\n",
        "\n",
        "  <img src=\"https://pytorch.org/assets/images/resnext.png\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "* Above is ResNEXT 50 (in the right) with Cardinality set to 32 and 4d i.e. residual layer channels start with 128 (4xC=4x32=128). A similar model is used in MiDaS but more parameters i.e. ResNEXT101 with 8d i.e. residual layer channels start with 256 (8xC=8x32=256). This has 4 residual layers.\n",
        "\n",
        "* Following is a simplified diagram of its architecture:\n",
        "  <img src=\"images/MiDaS.png" alt=\"drawing\" width=\"1000\"/>\n",
        "* Above is a good base model to start with i.e. use the pretrained model of MiDaS network and then train the other parts of the final model. MiDaS is chosen as base model for two reasons:\n",
        "  1. ResNeXt is a common network across all the three models\n",
        "  2. Training code for MiDaS is not available, hence it is good to use the pretrained weights for this model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSBZx8nK6THS"
      },
      "source": [
        "#### **YoloV3**\n",
        "\n",
        "* Following is the architecure of YoloV3:\n",
        "<img src=\"images/YoloV3Arch.png" alt=\"drawing\" width=\"1000\"/>\n",
        "* As seen above, it is using ResNet. We'll use ResNeXt branch of MiDaS network and then add three branches to get three scales of output of Yolo network\n",
        "* So the ResNeXt branch will have pretrained weights of MiDaS network and the output branches will be initialised with pretrained weights of YoloV3 and will later be fine tuned as part of combined training\n",
        "* Yolo output branches will be connected to ResNeXt branch with additional convolutional layers which will be trained as part of training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmzE7Z6D7voU"
      },
      "source": [
        "#### **PlaneRCNN**\n",
        "\n",
        "* Following is a simplified architecture of PlaneRCNN model:\n",
        "<img src=\"images/MaskRCNN.png" alt=\"drawing\" width=\"1000\"/>\n",
        "* As seen above it is a combination of multiple network, but we have common ResNet network here. Hence we can extend MiDaS's ResNeXt branch to incorporate planeRCNN network\n",
        "* Similar to YoloV3 model, we will load MiDaS's ResNeXt branch with pretrained model and all the other parts of planeRCNN model will have its own pretrained weights which will be trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONXRABgzAp1X"
      },
      "source": [
        "## Step 2: Load all the required code for building a combined model of MiDaS, YoloV3, PlanerRCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckr8aX9HpMVW"
      },
      "source": [
        "!git clone https://github.com/ashxjain/planercnn\n",
        "!git clone https://github.com/intel-isl/MiDaS.git\n",
        "!git clone https://github.com/ashxjain/YoloV3.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P06znZWvVe9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk2XKOx9-bIC"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpvAziurM2md"
      },
      "source": [
        "## Step 3: Install required packages for PlaneRCNN model and load its weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgHecLQnpZHh"
      },
      "source": [
        "%%shell\n",
        "cd /content/planercnn/roialign; python setup.py install; ./test.sh\n",
        "cp -r /usr/local/lib/python3.6/dist-packages/roi_align-0.0.2-py3.6-linux-x86_64.egg/roi_align /usr/local/lib/python3.6/dist-packages/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liyOPmDrvdhP"
      },
      "source": [
        "* Download model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OARjtUfBu8po"
      },
      "source": [
        "%%shell\n",
        "cd /content/planercnn/\n",
        "mkdir -p checkpoint\n",
        "\n",
        "\n",
        "#wget https://www.dropbox.com/s/yjcg6s57n581sk0/checkpoint.zip?dl=0\n",
        "#mv \"checkpoint.zip?dl=0\" \"planercnn_refine.zip\"\n",
        "\n",
        "fileId=1o2wZG0swF-HImZbQGPC7cHONkCThFVQZ\n",
        "fileName=planercnn_refine.zip\n",
        "curl -sc /tmp/cookie \"https://drive.google.com/uc?export=download&id=${fileId}\" > /dev/null\n",
        "code=\"$(awk '/_warning_/ {print $NF}' /tmp/cookie)\"  \n",
        "curl -Lb /tmp/cookie \"https://drive.google.com/uc?export=download&confirm=${code}&id=${fileId}\" -o ${fileName}\n",
        "\n",
        "mv planercnn_refine.zip checkpoint/\n",
        "cd checkpoint/\n",
        "unzip planercnn_refine.zip\n",
        "rm planercnn_refine.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2IU2wyzBl4o"
      },
      "source": [
        "* Verify PlaneRCNN model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYEUfBKZv9QE"
      },
      "source": [
        "%%shell\n",
        "cd /content/planercnn\n",
        "echo \"Running Tests\"\n",
        "python evaluate.py --methods=f --suffix=warping_refine --dataset=inference --customDataFolder=example_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBuLtxbMB-rM"
      },
      "source": [
        "* Load PlaneRCNN config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKDyI47coRET"
      },
      "source": [
        "%cd /content/planercnn\n",
        "\n",
        "import sys\n",
        "\n",
        "from config import InferenceConfig, PlaneConfig\n",
        "from options import parse_args\n",
        "from evaluate import PlaneRCNNDetector\n",
        "from datasets.plane_stereo_dataset import PlaneDataset\n",
        "from datasets.inference_dataset import InferenceDataset\n",
        "from models.model import SamePad2d, MaskRCNN\n",
        "from models.refinement_net import RefineModel\n",
        "\n",
        "sys.argv[1:] = [\"--methods=f\", \"--suffix=warping_refine\", \"--dataset=inference\", \"--customDataFolder=example_images\"]\n",
        "options = parse_args()\n",
        "plane_config = PlaneConfig(options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iClsq4tSVJ9t"
      },
      "source": [
        "## Step 4: Load all the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzbZgwXHvhmt"
      },
      "source": [
        "%cd /content\n",
        "from MiDaS.midas.midas_net import MidasNet\n",
        "from MiDaS.midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
        "import MiDaS.utils as midas_utils\n",
        "from torch import nn\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEp3R6TCzdPj"
      },
      "source": [
        "%cd /content/YoloV3\n",
        "from models import Darknet, YOLOLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4YgaQ7HChgJ"
      },
      "source": [
        "* Download MiDaS model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6CJWEPQGIrK"
      },
      "source": [
        "!wget https://github.com/intel-isl/MiDaS/releases/download/v2_1/model-f6b98070.pt -O /content/model-f6b98070.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp7Wsy0ij6Me"
      },
      "source": [
        "class FinalModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FinalModel, self).__init__()\n",
        "\n",
        "    # MiDaS Layers\n",
        "    # ============\n",
        "    # Load MiDas Model and its weights\n",
        "    midas_model = MidasNet('/content/model-f6b98070.pt', non_negative=True).to(device)\n",
        "    # Freeze all layers of MiDaS network\n",
        "    for param in midas_model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    self.resNext = midas_model.pretrained\n",
        "    self.midas_scratch = midas_model.scratch\n",
        "\n",
        "    # YoloV3 Layers\n",
        "    # =============\n",
        "    yolo_model = Darknet(\"/content/YoloV3/cfg/yolov3-ppe.cfg\", img_size=512).to(device)\n",
        "    #yolo_chkpt = torch.load('/content/YoloV3/weights/yolov3-spp-ultralytics.pt', map_location=device)\n",
        "    yolo_chkpt = torch.load('/content/YoloV3/weights/yolov3_best_300.pt', map_location=device)\n",
        "\n",
        "    # load model weights for Darknet\n",
        "    yolo_chkpt['model'] = {k: v for k, v in yolo_chkpt['model'].items() if yolo_model.state_dict()[k].numel() == v.numel()}\n",
        "    yolo_model.load_state_dict(yolo_chkpt['model'], strict=False)\n",
        "\n",
        "    anchors = [(10,13),  (16,30),  (33,23),  (30,61),  (62,45),  (59,119),  (116,90),  (156,198),  (373,326)]\n",
        "    numclasses = 4\n",
        "\n",
        "    self.yolo_start_1 = nn.Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
        "    self.yolo_start_2 = nn.Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
        "    self.yolo_start_3 = nn.Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
        "    # Following are used to fetch anchor vectors\n",
        "    self.yolo_layers = yolo_model.yolo_layers\n",
        "    self.module_list = yolo_model.module_list\n",
        "    for i, j in enumerate(self.yolo_layers):\n",
        "      # get number of grid points and anchor vec for this yolo layer\n",
        "      self.module_list[j].anchor_vec = self.module_list[j].anchor_vec.to(device)\n",
        "    \n",
        "    # Yolo scale-1\n",
        "    self.yolo_scale_1_conv_upsample = nn.Sequential(\n",
        "        nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "        nn.BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True),\n",
        "        nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "        nn.Upsample(scale_factor=2.0),\n",
        "        )\n",
        "    self.yolo_scale_1_pretrained = nn.Sequential(*list(yolo_model.module_list)[106:113])\n",
        "    self.yolo_scale_1_out = YOLOLayer(anchors[:3], numclasses, 512, -1, [], 1)\n",
        "    # Yolo scale-2\n",
        "    self.yolo_scale_2_conv_upsample = nn.Sequential(\n",
        "        nn.Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "        nn.BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True),\n",
        "        nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "        nn.Upsample(scale_factor=2.0),\n",
        "    )\n",
        "    self.yolo_scale_2_pretrained = nn.Sequential(*list(yolo_model.module_list)[94:100])\n",
        "    self.yolo_scale_2_out = YOLOLayer(anchors[3:6], numclasses, 512, -1, [], 1)\n",
        "\n",
        "    self.yolo_scale_2_pretrained_last = yolo_model.module_list[100]\n",
        "    # Yolo scale-3\n",
        "    self.yolo_scale_3_conv = nn.Conv2d(1024, 27, kernel_size=(1, 1), stride=(1, 1))\n",
        "    self.yolo_scale_3_out = YOLOLayer(anchors[6:], numclasses, 512, -1, [], 1)\n",
        "\n",
        "    # PlaneRCNN Layers\n",
        "    # ================\n",
        "    # Load MaskRCNN model\n",
        "    maskrcnn_model = MaskRCNN(plane_config)\n",
        "    # TODO: Load MaskRCNN model weights\n",
        "    checkpoint_dir = '/content/planercnn/checkpoint/planercnn_' + options.anchorType\n",
        "    if options.suffix != '':\n",
        "      checkpoint_dir += '_' + options.suffix\n",
        "    maskrcnn_state_dict = torch.load(checkpoint_dir + '/checkpoint.pth')\n",
        "    maskrcnn_model.load_state_dict(maskrcnn_state_dict)\n",
        "    \n",
        "    # Load Refine Model\n",
        "    refine_model = RefineModel(options)\n",
        "    # Load Refine Model weights\n",
        "    refine_model.load_state_dict(torch.load(checkpoint_dir + '/checkpoint_refine.pth'))\n",
        "\n",
        "    self.planercnn_maskrcnn_fpn = maskrcnn_model.fpn\n",
        "    self.planercnn_refine_model = refine_model\n",
        "\n",
        "  def forward(self, x, augment=False):\n",
        "    if augment:  # Augment images (inference and test only)\n",
        "      img_size = x.shape[-2:]  # height, width\n",
        "      s = [0.83, 0.67]  # scales\n",
        "      y = []\n",
        "      for i, xi in enumerate((x,\n",
        "                              torch_utils.scale_img(x.flip(3), s[0], same_shape=False),  # flip-lr and scale\n",
        "                              torch_utils.scale_img(x, s[1], same_shape=False),  # scale\n",
        "                              )):\n",
        "          # cv2.imwrite('img%g.jpg' % i, 255 * xi[0].numpy().transpose((1, 2, 0))[:, :, ::-1])\n",
        "          y.append(self.forward_once(xi)[0])\n",
        "\n",
        "      y[1][..., :4] /= s[0]  # scale\n",
        "      y[1][..., 0] = img_size[1] - y[1][..., 0]  # flip lr\n",
        "      y[2][..., :4] /= s[1]  # scale\n",
        "\n",
        "      y = torch.cat(y, 1)\n",
        "      return y, None\n",
        "    return self.forward_once(x)\n",
        "\n",
        "  def forward_once(self, x):\n",
        "    # MiDaS\n",
        "    # =====\n",
        "    resNext_layer_1 = self.resNext.layer1(x)\n",
        "    resNext_layer_2 = self.resNext.layer2(resNext_layer_1)\n",
        "    resNext_layer_3 = self.resNext.layer3(resNext_layer_2)\n",
        "    resNext_layer_4 = self.resNext.layer4(resNext_layer_3)\n",
        "\n",
        "    layer_1_rn = self.midas_scratch.layer1_rn(resNext_layer_1)\n",
        "    layer_2_rn = self.midas_scratch.layer2_rn(resNext_layer_2)\n",
        "    layer_3_rn = self.midas_scratch.layer3_rn(resNext_layer_3)\n",
        "    layer_4_rn = self.midas_scratch.layer4_rn(resNext_layer_4)\n",
        "\n",
        "    path_4 = self.midas_scratch.refinenet4(layer_4_rn)\n",
        "    path_3 = self.midas_scratch.refinenet3(path_4, layer_3_rn)\n",
        "    path_2 = self.midas_scratch.refinenet2(path_3, layer_2_rn)\n",
        "    path_1 = self.midas_scratch.refinenet1(path_2, layer_1_rn)\n",
        "\n",
        "    # MiDAS out\n",
        "    midas_out = self.midas_scratch.output_conv(path_1) # Output: 224x224x1\n",
        "    midas_out = torch.squeeze(midas_out, dim=1)\n",
        "\n",
        "    # YoloV3\n",
        "    # ======\n",
        "    yolo_out = []\n",
        "    # Yolo Scale-3 out\n",
        "    yolo_scale3_start3 = self.yolo_start_3(resNext_layer_4)\n",
        "    yolo_scale3_conv = self.yolo_scale_3_conv(yolo_scale3_start3)\n",
        "    yolo_scale3_out = self.yolo_scale_3_out(yolo_scale3_conv, None) # Output: 13x13x27\n",
        "    yolo_out.append(yolo_scale3_out)\n",
        "\n",
        "    # Yolo Scale-2 Out\n",
        "    yolo_scale2_start3 = self.yolo_start_3(resNext_layer_4)\n",
        "    yolo_scale2_conv_upsample = self.yolo_scale_2_conv_upsample(yolo_scale2_start3)\n",
        "    yolo_scale2_start2 = self.yolo_start_2(resNext_layer_3)\n",
        "    yolo_scale2_cat = torch.cat([yolo_scale2_start2, yolo_scale2_conv_upsample], 1)\n",
        "    yolo_scale2_pretrained = self.yolo_scale_2_pretrained(yolo_scale2_cat)\n",
        "    yolo_scale2 = self.yolo_scale_2_pretrained_last(yolo_scale2_pretrained)\n",
        "    yolo_scale2_out = self.yolo_scale_2_out(yolo_scale2, None) # Output: 26x26x27\n",
        "    yolo_out.append(yolo_scale2_out)\n",
        "\n",
        "    # Yolo Scale-1 Out\n",
        "    yolo_scale1_conv_upsample = self.yolo_scale_1_conv_upsample(yolo_scale2_pretrained)\n",
        "    yolo_scale1_start = self.yolo_start_1(resNext_layer_2)\n",
        "    yolo_scale1 = torch.cat([yolo_scale1_start, yolo_scale1_conv_upsample], 1)\n",
        "    yolo_scale1_pretrained = self.yolo_scale_1_pretrained(yolo_scale1)\n",
        "    yolo_scale1_out = self.yolo_scale_1_out(yolo_scale1_pretrained, None) # Output: 52x52x27\n",
        "    yolo_out.append(yolo_scale1_out)\n",
        "\n",
        "    # PlaneRCNN\n",
        "    # =========\n",
        "    # MaskRCNN FPN\n",
        "    c2_out = resNext_layer_1\n",
        "    c3_out = resNext_layer_2\n",
        "    c4_out = resNext_layer_3\n",
        "    p5_out = self.P5_conv1(resNext_layer_4)\n",
        "    if self.planercnn_maskrcnn_fpn.bilinear_upsampling:\n",
        "        p4_out = self.planercnn_maskrcnn_fpn.P4_conv1(c4_out) + F.upsample(p5_out, scale_factor=2, mode='bilinear')\n",
        "        p3_out = self.planercnn_maskrcnn_fpn.P3_conv1(c3_out) + F.upsample(p4_out, scale_factor=2, mode='bilinear')\n",
        "        p2_out = self.planercnn_maskrcnn_fpn.P2_conv1(c2_out) + F.upsample(p3_out, scale_factor=2, mode='bilinear')\n",
        "    else:\n",
        "        p4_out = self.planercnn_maskrcnn_fpn.P4_conv1(c4_out) + F.upsample(p5_out, scale_factor=2)\n",
        "        p3_out = self.planercnn_maskrcnn_fpn.P3_conv1(c3_out) + F.upsample(p4_out, scale_factor=2)\n",
        "        p2_out = self.planercnn_maskrcnn_fpn.P2_conv1(c2_out) + F.upsample(p3_out, scale_factor=2)\n",
        "        pass\n",
        "    p5_out = self.planercnn_maskrcnn_fpn.P5_conv2(p5_out)\n",
        "    p4_out = self.planercnn_maskrcnn_fpn.P4_conv2(p4_out)\n",
        "    p3_out = self.planercnn_maskrcnn_fpn.P3_conv2(p3_out)\n",
        "    p2_out = self.planercnn_maskrcnn_fpn.P2_conv2(p2_out)\n",
        "    ## P6 is used for the 5th anchor scale in RPN. Generated by\n",
        "    ## subsampling from P5 with stride of 2.\n",
        "    p6_out = self.planercnn_maskrcnn_fpn.P6(p5_out)\n",
        "\n",
        "    rpn_feature_maps = [p2_out, p3_out, p4_out, p5_out, p6_out]\n",
        "    mrcnn_feature_maps = [p2_out, p3_out, p4_out, p5_out]\n",
        "    feature_maps = [feature_map for index, feature_map in enumerate(rpn_feature_maps[::-1])]\n",
        "    ## Loop through pyramid layers\n",
        "    layer_outputs = []  ## list of lists\n",
        "    for p in rpn_feature_maps:\n",
        "        layer_outputs.append(self.rpn(p))\n",
        "    ## Concatenate layer outputs\n",
        "    ## Convert from list of lists of level outputs to list of lists\n",
        "    ## of outputs across levels.\n",
        "    ## e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
        "    outputs = list(zip(*layer_outputs))\n",
        "    outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
        "    rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
        "\n",
        "    ## Generate proposals\n",
        "    ## Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
        "    ## and zero padded.\n",
        "    proposal_count = self.config.POST_NMS_ROIS_TRAINING if 'training' in mode and use_refinement == False \\\n",
        "        else self.config.POST_NMS_ROIS_INFERENCE\n",
        "    rpn_rois = proposal_layer([rpn_class, rpn_bbox],\n",
        "                              proposal_count=proposal_count,\n",
        "                              nms_threshold=self.config.RPN_NMS_THRESHOLD,\n",
        "                              anchors=self.anchors,\n",
        "                              config=self.config)\n",
        "\n",
        "    if mode == 'inference':\n",
        "        ## Network Heads\n",
        "        ## Proposal classifier and BBox regressor heads\n",
        "        mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_parameters = self.classifier(mrcnn_feature_maps, rpn_rois, ranges)\n",
        "\n",
        "        ## Detections\n",
        "        ## output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\n",
        "        detections = detection_layer(self.config, rpn_rois, mrcnn_class, mrcnn_bbox, mrcnn_parameters, image_metas)\n",
        "\n",
        "        if len(detections) == 0:\n",
        "            return [[]], [[]], depth_np\n",
        "        ## Convert boxes to normalized coordinates\n",
        "        ## TODO: let DetectionLayer return normalized coordinates to avoid\n",
        "        ##       unnecessary conversions\n",
        "        h, w = self.config.IMAGE_SHAPE[:2]\n",
        "        scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
        "        if self.config.GPU_COUNT:\n",
        "            scale = scale.cuda()\n",
        "        detection_boxes = detections[:, :4] / scale\n",
        "\n",
        "        ## Add back batch dimension\n",
        "        detection_boxes = detection_boxes.unsqueeze(0)\n",
        "\n",
        "        ## Create masks for detections\n",
        "        mrcnn_mask, roi_features = self.mask(mrcnn_feature_maps, detection_boxes)\n",
        "\n",
        "        ## Add back batch dimension\n",
        "        detections = detections.unsqueeze(0)\n",
        "        mrcnn_mask = mrcnn_mask.unsqueeze(0)\n",
        "        planercnn_out = (detections, mrcnn_mask)\n",
        "    else:\n",
        "      gt_class_ids = input[2]\n",
        "      gt_boxes = input[3]\n",
        "      gt_masks = input[4]\n",
        "      gt_parameters = input[5]\n",
        "\n",
        "      ## Normalize coordinates\n",
        "      h, w = self.config.IMAGE_SHAPE[:2]\n",
        "      scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
        "      if self.config.GPU_COUNT:\n",
        "          scale = scale.cuda()\n",
        "      gt_boxes = gt_boxes / scale\n",
        "\n",
        "      ## Generate detection targets\n",
        "      ## Subsamples proposals and generates target outputs for training\n",
        "      ## Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
        "      ## padded. Equally, returned rois and targets are zero padded.\n",
        "      rois, target_class_ids, target_deltas, target_mask, target_parameters = \\\n",
        "          detection_target_layer(rpn_rois, gt_class_ids, gt_boxes, gt_masks, gt_parameters, self.config)\n",
        "\n",
        "      if len(rois) == 0:\n",
        "          mrcnn_class_logits = Variable(torch.FloatTensor())\n",
        "          mrcnn_class = Variable(torch.IntTensor())\n",
        "          mrcnn_bbox = Variable(torch.FloatTensor())\n",
        "          mrcnn_mask = Variable(torch.FloatTensor())\n",
        "          mrcnn_parameters = Variable(torch.FloatTensor())\n",
        "          if self.config.GPU_COUNT:\n",
        "              mrcnn_class_logits = mrcnn_class_logits.cuda()\n",
        "              mrcnn_class = mrcnn_class.cuda()\n",
        "              mrcnn_bbox = mrcnn_bbox.cuda()\n",
        "              mrcnn_mask = mrcnn_mask.cuda()\n",
        "              mrcnn_parameters = mrcnn_parameters.cuda()\n",
        "      else:\n",
        "          ## Network Heads\n",
        "          ## Proposal classifier and BBox regressor heads\n",
        "          #print([maps.shape for maps in mrcnn_feature_maps], target_parameters.shape)\n",
        "          mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_parameters = self.classifier(mrcnn_feature_maps, rois, ranges, target_parameters)\n",
        "\n",
        "          ## Create masks for detections\n",
        "          mrcnn_mask, _ = self.mask(mrcnn_feature_maps, rois)\n",
        "\n",
        "      planercnn_out = [rpn_class_logits, rpn_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask, target_parameters, mrcnn_parameters, rois, depth_np]\n",
        "\n",
        "    return [midas_out, yolo_out, planercnn_out]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2rgqK7GUwug"
      },
      "source": [
        "## Step 5: Training YoloV3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzp-eTwXfCA"
      },
      "source": [
        "#### Fetch YoloV3 specific configs and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fN2p18Z8GQ9"
      },
      "source": [
        "%%shell\n",
        "cd /content\n",
        "ls -altrh drive/My\\ Drive/EVA/Datasets/YoloV3-PPE/YoloV3_Dataset.zip\n",
        "\n",
        "unzip drive/My\\ Drive/EVA/Datasets/YoloV3-PPE/YoloV3_Dataset.zip\n",
        "\n",
        "srcFolder=\"YoloV3_Dataset\"\n",
        "targetDataFolder=\"YoloV3/data/ppedata\"\n",
        "mkdir -p $targetDataFolder\n",
        "# create ppe.data file\n",
        "cat > $targetDataFolder/ppe.data <<EOF\n",
        "classes=4\n",
        "train=data/ppedata/ppetrain.txt\n",
        "valid=data/ppedata/ppetest.txt \n",
        "names=data/ppedata/ppe.names\n",
        "EOF\n",
        "\n",
        "# copy required files from dataset\n",
        "cp $srcFolder/classes.txt $targetDataFolder/ppe.names\n",
        "mkdir -p $targetDataFolder/images && cp $srcFolder/Images/* $targetDataFolder/images/\n",
        "mkdir -p $targetDataFolder/labels && cp $srcFolder/Labels/* $targetDataFolder/labels/\n",
        "# Make sure any decimals around 1 i.e 1.xxxxx should be rounded off to 1.0\n",
        "sed -i 's/1\\.[0-9]*/1.0/g'  $targetDataFolder/labels/*\n",
        "\n",
        "echo \"> ppe.data\"\n",
        "cat $targetDataFolder/ppe.data\n",
        "\n",
        "echo \"\"; echo \"> ppe.names\"\n",
        "cat $targetDataFolder/ppe.names; echo\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhXOG775zPv7"
      },
      "source": [
        "%%shell\n",
        "cd /content\n",
        "# Copy the contents of 'yolov3-spp.cfg' file to a new file called 'yolov3-ppe.cfg' file in the data/cfg folder.\n",
        "cp YoloV3/cfg/yolov3-spp.cfg YoloV3/cfg/yolov3-ppe.cfg\n",
        "\n",
        "# Search for 'filters=255' (you should get entries entries). Change 255 to *27* = (4+1+4)*3\n",
        "sed -i 's/filters=255/filters=27/g' YoloV3/cfg/yolov3-ppe.cfg\n",
        "\n",
        "# Search for 'classes=80' and change all three entries to 'classes=4'\n",
        "sed -i 's/classes=80/classes=4/g' YoloV3/cfg/yolov3-ppe.cfg\n",
        "\n",
        "sed -i 's/burn_in.*/burn_in=100/g' YoloV3/cfg/yolov3-ppe.cfg\n",
        "sed -i 's/max_batches.*/max_batches=5000/g' YoloV3/cfg/yolov3-ppe.cfg\n",
        "sed -i 's/steps=.*/steps=4000,4500/g' YoloV3/cfg/yolov3-ppe.cfg\n",
        "\n",
        "# Verify if changes took place successfully\n",
        "grep \"filters=27\" YoloV3/cfg/yolov3-ppe.cfg\n",
        "grep \"classes\" YoloV3/cfg/yolov3-ppe.cfg\n",
        "grep \"burn_in\" YoloV3/cfg/yolov3-ppe.cfg\n",
        "grep \"max_batches\" YoloV3/cfg/yolov3-ppe.cfg\n",
        "grep \"steps=\" YoloV3/cfg/yolov3-ppe.cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVmesG1y9oF7"
      },
      "source": [
        "# Create a folder called weights in the root (YoloV3) folder\n",
        "!mkdir -p /content/YoloV3/weights\n",
        "!cp /content/drive/My\\ Drive/EVA/Models/YoloV3PPE/*.pt /content/YoloV3/weights/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M97df_G_8qdU"
      },
      "source": [
        "%cd /content\n",
        "\n",
        "import re\n",
        "import os\n",
        "import imagesize\n",
        "\n",
        "srcFolder=\"YoloV3_Dataset\"\n",
        "targetDataFolder=\"YoloV3/data/ppedata\"\n",
        "trainFile = open(f'{targetDataFolder}/ppetrain.txt', 'w')\n",
        "testFile = open(f'{targetDataFolder}/ppetest.txt', 'w')\n",
        "trainShapesFile = open(f'{targetDataFolder}/ppetrain.shapes', 'w')\n",
        "testShapesFile = open(f'{targetDataFolder}/ppetest.shapes', 'w')\n",
        "labelFiles = os.listdir(srcFolder + \"/Labels\")\n",
        "imgFiles = os.listdir(srcFolder + \"/Images\")\n",
        "count = 0\n",
        "testCnt = len(labelFiles)/10\n",
        "trainCnt = len(labelFiles) - testCnt\n",
        "for file in labelFiles:\n",
        "  imgParts = file.split(\".txt\")\n",
        "  r = re.compile(re.escape(imgParts[0])+\".*\")\n",
        "  found = False\n",
        "  for imgFile in imgFiles:\n",
        "    if r.match(imgFile):\n",
        "      shape = imagesize.get(f'{srcFolder}/Images/{imgFile}')\n",
        "      if count < trainCnt:\n",
        "        trainFile.write(f'./data/ppedata/images/{imgFile}\\n')\n",
        "        if shape: trainShapesFile.write(f'{shape[0]} {shape[1]}\\n')\n",
        "      else:\n",
        "        testFile.write(f'./data/ppedata/images/{imgFile}\\n')\n",
        "        if shape: testShapesFile.write(f'{shape[0]} {shape[1]}\\n')\n",
        "      count+=1\n",
        "      found = True\n",
        "      break\n",
        "trainFile.close()\n",
        "testFile.close()\n",
        "trainShapesFile.close()\n",
        "testShapesFile.close()\n",
        "\n",
        "!echo \"Total train images: $(cat YoloV3/data/ppedata/ppetrain.txt | wc -l)\"\n",
        "!echo \"Total train image shapes: $(cat YoloV3/data/ppedata/ppetrain.shapes | wc -l)\"\n",
        "!echo \"Top 5 lines of train file: $(head -n 5 YoloV3/data/ppedata/ppetrain.txt)\"\n",
        "!echo \"\";\n",
        "!echo \"Total test images: $(cat YoloV3/data/ppedata/ppetest.txt | wc -l)\"\n",
        "!echo \"Total test image shapes: $(cat YoloV3/data/ppedata/ppetest.shapes | wc -l)\"\n",
        "!echo \"Top 5 lines of test file: $(head -n 5 YoloV3/data/ppedata/ppetest.txt)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uiRNfIrlJjv"
      },
      "source": [
        "def yolo_test(cfg,\n",
        "         data,\n",
        "         weights=None,\n",
        "         batch_size=16,\n",
        "         img_size=416,\n",
        "         conf_thres=0.001,\n",
        "         iou_thres=0.6,  # for nms\n",
        "         save_json=False,\n",
        "         single_cls=False,\n",
        "         augment=False,\n",
        "         model=None,\n",
        "         dataloader=None):\n",
        "    # Initialize/load model and set device\n",
        "    if model is None:\n",
        "        device = torch_utils.select_device(opt.device, batch_size=batch_size)\n",
        "        verbose = opt.task == 'test'\n",
        "\n",
        "        # Remove previous\n",
        "        for f in glob.glob('test_batch*.png'):\n",
        "            os.remove(f)\n",
        "\n",
        "        # Initialize model\n",
        "        model = FinalModel().to(device)\n",
        "        model.to(device)\n",
        "\n",
        "        if device.type != 'cpu' and torch.cuda.device_count() > 1:\n",
        "            model = nn.DataParallel(model)\n",
        "    else:  # called by train.py\n",
        "        device = next(model.parameters()).device  # get model device\n",
        "        verbose = False\n",
        "\n",
        "    # Configure run\n",
        "    data = parse_data_cfg(data)\n",
        "    nc = 1 if single_cls else int(data['classes'])  # number of classes\n",
        "    path = data['valid']  # path to test images\n",
        "    names = load_classes(data['names'])  # class names\n",
        "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
        "    iouv = iouv[0].view(1)  # comment for mAP@0.5:0.95\n",
        "    niou = iouv.numel()\n",
        "\n",
        "    # Dataloader\n",
        "    if dataloader is None:\n",
        "        dataset = LoadImagesAndLabels(path, img_size, batch_size, rect=True, single_cls=opt.single_cls)\n",
        "        batch_size = min(batch_size, len(dataset))\n",
        "        dataloader = DataLoader(dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                num_workers=min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8]),\n",
        "                                pin_memory=True,\n",
        "                                collate_fn=dataset.collate_fn)\n",
        "\n",
        "    seen = 0\n",
        "    model.eval()\n",
        "    _ = model(torch.zeros((1, 3, img_size, img_size), device=device)) if device.type != 'cpu' else None  # run once\n",
        "    coco91class = coco80_to_coco91_class()\n",
        "    s = ('%20s' + '%10s' * 6) % ('Class', 'Images', 'Targets', 'P', 'R', 'mAP@0.5', 'F1')\n",
        "    p, r, f1, mp, mr, map, mf1, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
        "    loss = torch.zeros(3, device=device)\n",
        "    jdict, stats, ap, ap_class = [], [], [], []\n",
        "    for batch_i, (imgs, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
        "        imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n",
        "        targets = targets.to(device)\n",
        "        nb, _, height, width = imgs.shape  # batch size, channels, height, width\n",
        "        whwh = torch.Tensor([width, height, width, height]).to(device)\n",
        "\n",
        "        # Plot images with bounding boxes\n",
        "        f = 'test_batch%g.png' % batch_i  # filename\n",
        "        if batch_i < 1 and not os.path.exists(f):\n",
        "            plot_images(imgs=imgs, targets=targets, paths=paths, fname=f)\n",
        "\n",
        "        # Disable gradients\n",
        "        with torch.no_grad():\n",
        "            # Run model\n",
        "            t = torch_utils.time_synchronized()\n",
        "            midas_out, yolo_out = model(imgs, augment=augment)  # inference and training outputs\n",
        "            t0 += torch_utils.time_synchronized() - t\n",
        "            inf_out, train_out = zip(*yolo_out)\n",
        "            inf_out = torch.cat(inf_out, 1)\n",
        "\n",
        "            # Compute loss\n",
        "            if hasattr(model, 'hyp'):  # if model has loss hyperparameters\n",
        "                loss += compute_loss(train_out, targets, model)[1][:3]  # GIoU, obj, cls\n",
        "\n",
        "            # Run NMS\n",
        "            t = torch_utils.time_synchronized()\n",
        "            output = non_max_suppression(inf_out, conf_thres=conf_thres, iou_thres=iou_thres)  # nms\n",
        "            t1 += torch_utils.time_synchronized() - t\n",
        "\n",
        "        # Statistics per image\n",
        "        for si, pred in enumerate(output):\n",
        "            labels = targets[targets[:, 0] == si, 1:]\n",
        "            nl = len(labels)\n",
        "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
        "            seen += 1\n",
        "\n",
        "            if pred is None:\n",
        "                if nl:\n",
        "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
        "                continue\n",
        "\n",
        "            # Append to text file\n",
        "            # with open('test.txt', 'a') as file:\n",
        "            #    [file.write('%11.5g' * 7 % tuple(x) + '\\n') for x in pred]\n",
        "\n",
        "            # Clip boxes to image bounds\n",
        "            clip_coords(pred, (height, width))\n",
        "\n",
        "            # Append to pycocotools JSON dictionary\n",
        "            if save_json:\n",
        "                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
        "                image_id = int(Path(paths[si]).stem.split('_')[-1])\n",
        "                box = pred[:, :4].clone()  # xyxy\n",
        "                scale_coords(imgs[si].shape[1:], box, shapes[si][0], shapes[si][1])  # to original shape\n",
        "                box = xyxy2xywh(box)  # xywh\n",
        "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
        "                for p, b in zip(pred.tolist(), box.tolist()):\n",
        "                    jdict.append({'image_id': image_id,\n",
        "                                  'category_id': coco91class[int(p[5])],\n",
        "                                  'bbox': [round(x, 3) for x in b],\n",
        "                                  'score': round(p[4], 5)})\n",
        "\n",
        "            # Assign all predictions as incorrect\n",
        "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
        "            if nl:\n",
        "                detected = []  # target indices\n",
        "                tcls_tensor = labels[:, 0]\n",
        "\n",
        "                # target boxes\n",
        "                tbox = xywh2xyxy(labels[:, 1:5]) * whwh\n",
        "\n",
        "                # Per target class\n",
        "                for cls in torch.unique(tcls_tensor):\n",
        "                    ti = (cls == tcls_tensor).nonzero().view(-1)  # prediction indices\n",
        "                    pi = (cls == pred[:, 5]).nonzero().view(-1)  # target indices\n",
        "\n",
        "                    # Search for detections\n",
        "                    if pi.shape[0]:\n",
        "                        # Prediction to target ious\n",
        "                        ious, i = box_iou(pred[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
        "\n",
        "                        # Append detections\n",
        "                        for j in (ious > iouv[0]).nonzero():\n",
        "                            d = ti[i[j]]  # detected target\n",
        "                            if d not in detected:\n",
        "                                detected.append(d)\n",
        "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
        "                                if len(detected) == nl:  # all targets already located in image\n",
        "                                    break\n",
        "\n",
        "            # Append statistics (correct, conf, pcls, tcls)\n",
        "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
        "\n",
        "    # Compute statistics\n",
        "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
        "    if len(stats):\n",
        "        p, r, ap, f1, ap_class = ap_per_class(*stats)\n",
        "        if niou > 1:\n",
        "            p, r, ap, f1 = p[:, 0], r[:, 0], ap.mean(1), ap[:, 0]  # [P, R, AP@0.5:0.95, AP@0.5]\n",
        "        mp, mr, map, mf1 = p.mean(), r.mean(), ap.mean(), f1.mean()\n",
        "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
        "    else:\n",
        "        nt = torch.zeros(1)\n",
        "\n",
        "    # Print results\n",
        "    pf = '%20s' + '%10.3g' * 6  # print format\n",
        "    print(pf % ('all', seen, nt.sum(), mp, mr, map, mf1))\n",
        "\n",
        "    # Print results per class\n",
        "    if verbose and nc > 1 and len(stats):\n",
        "        for i, c in enumerate(ap_class):\n",
        "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap[i], f1[i]))\n",
        "\n",
        "    # Print speeds\n",
        "    if verbose or save_json:\n",
        "        t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (img_size, img_size, batch_size)  # tuple\n",
        "        print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
        "\n",
        "\n",
        "    maps = np.zeros(nc) + map\n",
        "    for i, c in enumerate(ap_class):\n",
        "        maps[c] = ap[i]\n",
        "    return (mp, mr, map, mf1, *(loss.cpu() / len(dataloader)).tolist()), maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Y_pTI9zPrT"
      },
      "source": [
        "%cd /content/YoloV3\n",
        "\n",
        "import torch.distributed as dist\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import test  # import test.py to get mAP after each epoch\n",
        "from models import *\n",
        "from utils.datasets import *\n",
        "from utils.utils import *\n",
        "\n",
        "mixed_precision = True\n",
        "try:  # Mixed precision training https://github.com/NVIDIA/apex\n",
        "    from apex import amp\n",
        "except:\n",
        "    # print('Apex recommended for mixed precision and faster training: https://github.com/NVIDIA/apex')\n",
        "    mixed_precision = False  # not installed\n",
        "\n",
        "wdir = '/content/YoloV3/weights/' # weights dir\n",
        "last = wdir + 'last.pt'\n",
        "best = wdir + 'best.pt'\n",
        "results_file = 'results.txt'\n",
        "\n",
        "# Hyperparameters https://github.com/ultralytics/yolov3/issues/310\n",
        "\n",
        "hyp = {'giou': 3.54,  # giou loss gain\n",
        "       'cls': 37.4,  # cls loss gain\n",
        "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
        "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
        "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
        "       'iou_t': 0.225,  # iou training threshold\n",
        "       'lr0': 0.01,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
        "       'lrf': 0.0005,  # final learning rate (with cos scheduler)\n",
        "       'momentum': 0.937,  # SGD momentum\n",
        "       'weight_decay': 0.000484,  # optimizer weight decay\n",
        "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
        "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
        "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
        "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
        "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
        "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
        "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
        "       'shear': 0.641 * 0}  # image shear (+/- deg)\n",
        "\n",
        "def yolo_train():\n",
        "    cfg = opt_cfg\n",
        "    data = opt_data\n",
        "    epochs = opt_epochs  # 500200 batches at bs 64, 117263 images = 273 epochs\n",
        "    batch_size = opt_batch_size\n",
        "    accumulate = opt_accumulate  # effective bs = batch_size * accumulate = 16 * 4 = 64\n",
        "    weights = last if opt_resume else opt_weights # initial training weights\n",
        "    imgsz_min, imgsz_max, imgsz_test = opt_img_size  # img sizes (min, max, test)\n",
        "    multi_scale = opt_multi_scale\n",
        "    # Image Sizes\n",
        "    gs = 64  # (pixels) grid size\n",
        "    assert math.fmod(imgsz_min, gs) == 0, '--img-size %g must be a %g-multiple' % (imgsz_min, gs)\n",
        "    multi_scale |= imgsz_min != imgsz_max  # multi if different (min, max)\n",
        "    if multi_scale:\n",
        "        if imgsz_min == imgsz_max:\n",
        "            imgsz_min //= 1.5\n",
        "            imgsz_max //= 0.667\n",
        "        grid_min, grid_max = imgsz_min // gs, imgsz_max // gs\n",
        "        imgsz_min, imgsz_max = grid_min * gs, grid_max * gs\n",
        "    img_size = imgsz_max  # initialize with max size\n",
        "\n",
        "    # Configure run\n",
        "    init_seeds()\n",
        "    data_dict = parse_data_cfg(data)\n",
        "    train_path = data_dict['train']\n",
        "    test_path = data_dict['valid']\n",
        "    nc = int(data_dict['classes'])  # number of classes\n",
        "    hyp['cls'] *= nc / 80  # update coco-tuned hyp['cls'] to current dataset\n",
        "\n",
        "    # Remove previous results\n",
        "    for f in glob.glob('*_batch*.png') + glob.glob(results_file):\n",
        "        os.remove(f)\n",
        "\n",
        "    # Initialize model\n",
        "    model = FinalModel().to(device)\n",
        "    # Optimizer\n",
        "    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
        "    for k, v in dict(model.named_parameters()).items():\n",
        "        if '.bias' in k:\n",
        "            pg2 += [v]  # biases\n",
        "        elif 'Conv2d.weight' in k:\n",
        "            pg1 += [v]  # apply weight_decay\n",
        "        else:\n",
        "            pg0 += [v]  # all else\n",
        "\n",
        "    optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
        "    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
        "    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
        "    del pg0, pg1, pg2\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_fitness = 0.0\n",
        "    #attempt_download(weights)\n",
        "    if opt_resume and weights.endswith('.pt'):  # pytorch format\n",
        "        # possible weights are '*.pt', 'yolov3-spp.pt', 'yolov3-tiny.pt' etc.\n",
        "        chkpt = torch.load(weights, map_location=device)\n",
        "\n",
        "        # load model\n",
        "        try:\n",
        "            chkpt['model'] = {k: v for k, v in chkpt['model'].items() if model.state_dict()[k].numel() == v.numel()}\n",
        "            model.load_state_dict(chkpt['model'], strict=False)\n",
        "        except KeyError as e:\n",
        "            s = \"%s is not compatible with %s. Specify --weights '' or specify a --cfg compatible with %s. \" \\\n",
        "                \"See https://github.com/ultralytics/yolov3/issues/657\" % (opt.weights, opt.cfg, opt.weights)\n",
        "            raise KeyError(s) from e\n",
        "        \n",
        "        # load optimizer\n",
        "        if chkpt['optimizer'] is not None:\n",
        "            optimizer.load_state_dict(chkpt['optimizer'])\n",
        "            best_fitness = chkpt['best_fitness']\n",
        "\n",
        "        # load results\n",
        "        if chkpt.get('training_results') is not None:\n",
        "            with open(results_file, 'w') as file:\n",
        "                file.write(chkpt['training_results'])  # write results.txt\n",
        "\n",
        "        start_epoch = chkpt['epoch'] + 1\n",
        "        del chkpt\n",
        "\n",
        "    elif opt_resume and len(weights) > 0:  # darknet format\n",
        "        # possible weights are '*.weights', 'yolov3-tiny.conv.15',  'darknet53.conv.74' etc.\n",
        "        load_darknet_weights(model, weights)\n",
        "\n",
        "    # Mixed precision training https://github.com/NVIDIA/apex\n",
        "    if mixed_precision:\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
        "\n",
        "    # Scheduler https://github.com/ultralytics/yolov3/issues/238\n",
        "    lf = lambda x: (((1 + math.cos(\n",
        "        x * math.pi / epochs)) / 2) ** 1.0) * 0.95 + 0.05  # cosine https://arxiv.org/pdf/1812.01187.pdf\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf, last_epoch=start_epoch - 1)\n",
        "\n",
        "        # Initialize distributed training\n",
        "    if device.type != 'cpu' and torch.cuda.device_count() > 1 and torch.distributed.is_available():\n",
        "        dist.init_process_group(backend='nccl',  # 'distributed backend'\n",
        "                                init_method='tcp://127.0.0.1:9999',  # distributed training init method\n",
        "                                world_size=1,  # number of nodes for distributed training\n",
        "                                rank=0)  # distributed training node rank\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)\n",
        "        model.yolo_layers = model.module.yolo_layers  # move yolo layer indices to top level\n",
        "\n",
        "    # Dataset\n",
        "    dataset = LoadImagesAndLabels(train_path, img_size, batch_size,\n",
        "                                  augment=True,\n",
        "                                  hyp=hyp,  # augmentation hyperparameters\n",
        "                                  rect=opt_rect,  # rectangular training\n",
        "                                  cache_images=opt_cache_images,\n",
        "                                  single_cls=opt_single_cls)\n",
        "\n",
        "    # Dataloader\n",
        "    batch_size = min(batch_size, len(dataset))\n",
        "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             num_workers=nw,\n",
        "                                             shuffle=not opt_rect,  # Shuffle=True unless rectangular training is used\n",
        "                                             pin_memory=True,\n",
        "                                             collate_fn=dataset.collate_fn)\n",
        "\n",
        "    # Testloader\n",
        "    testloader = torch.utils.data.DataLoader(LoadImagesAndLabels(test_path, imgsz_test, batch_size,\n",
        "                                                                 hyp=hyp,\n",
        "                                                                 rect=True,\n",
        "                                                                 cache_images=opt_cache_images,\n",
        "                                                                 single_cls=opt_single_cls),\n",
        "                                             batch_size=batch_size,\n",
        "                                             num_workers=nw,\n",
        "                                             pin_memory=True,\n",
        "                                             collate_fn=dataset.collate_fn)\n",
        "\n",
        "    # Model parameters\n",
        "    model.nc = nc  # attach number of classes to model\n",
        "    model.hyp = hyp  # attach hyperparameters to model\n",
        "    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n",
        "    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n",
        "\n",
        "    # Model EMA\n",
        "    ema = torch_utils.ModelEMA(model)\n",
        "\n",
        "    # Start training\n",
        "    nb = len(dataloader)  # number of batches\n",
        "    n_burn = max(3 * nb, 500)  # burn-in iterations, max(3 epochs, 500 iterations)\n",
        "    maps = np.zeros(nc)  # mAP per class\n",
        "    # torch.autograd.set_detect_anomaly(True)\n",
        "    results = (0, 0, 0, 0, 0, 0, 0)  # 'P', 'R', 'mAP', 'F1', 'val GIoU', 'val Objectness', 'val Classification'\n",
        "    t0 = time.time()\n",
        "    print('Image sizes %g - %g train, %g test' % (imgsz_min, imgsz_max, imgsz_test))\n",
        "    print('Using %g dataloader workers' % nw)\n",
        "    print('Starting training for %g epochs...' % epochs)\n",
        "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
        "        model.train()\n",
        "\n",
        "        # Update image weights (optional)\n",
        "        if dataset.image_weights:\n",
        "            w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\n",
        "            image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n",
        "            dataset.indices = random.choices(range(dataset.n), weights=image_weights, k=dataset.n)  # rand weighted idx\n",
        "\n",
        "        mloss = torch.zeros(4).to(device)  # mean losses\n",
        "        print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n",
        "        pbar = tqdm(enumerate(dataloader), total=nb)  # progress bar\n",
        "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
        "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
        "            imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Burn-in\n",
        "            if ni <= n_burn * 2:\n",
        "                model.gr = np.interp(ni, [0, n_burn * 2], [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n",
        "                if ni == n_burn:  # burnin complete\n",
        "                    print_model_biases(model)\n",
        "\n",
        "                for j, x in enumerate(optimizer.param_groups):\n",
        "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
        "                    x['lr'] = np.interp(ni, [0, n_burn], [0.1 if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
        "                    if 'momentum' in x:\n",
        "                        x['momentum'] = np.interp(ni, [0, n_burn], [0.9, hyp['momentum']])\n",
        "\n",
        "            # Multi-Scale training\n",
        "            if opt_multi_scale:\n",
        "                if ni / accumulate % 1 == 0:  # adjust img_size (67% - 150%) every 1 batch\n",
        "                    img_size = random.randrange(grid_min, grid_max + 1) * gs\n",
        "                sf = img_size / max(imgs.shape[2:])  # scale factor\n",
        "                if sf != 1:\n",
        "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to 32-multiple)\n",
        "                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Run model\n",
        "            pred = model(imgs)\n",
        "            # Compute loss\n",
        "            loss, loss_items = compute_loss(pred[1], targets, model)\n",
        "            if not torch.isfinite(loss):\n",
        "                print('WARNING: non-finite loss, ending training ', loss_items)\n",
        "                return results\n",
        "\n",
        "            # Scale loss by nominal batch_size of 64\n",
        "            loss *= batch_size / 64\n",
        "\n",
        "            # Compute gradient\n",
        "            if mixed_precision:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            # Optimize accumulated gradient\n",
        "            if ni % accumulate == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                ema.update(model)\n",
        "\n",
        "            # Print batch results\n",
        "            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
        "            mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n",
        "            s = ('%10s' * 2 + '%10.3g' * 6) % ('%g/%g' % (epoch, epochs - 1), mem, *mloss, len(targets), img_size)\n",
        "            pbar.set_description(s)\n",
        "\n",
        "            # end batch ------------------------------------------------------------------------------------------------\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Process epoch results\n",
        "        ema.update_attr(model)\n",
        "        final_epoch = epoch + 1 == epochs\n",
        "\n",
        "        if not opt_notest or final_epoch:  # Calculate mAP\n",
        "            is_coco = any([x in data for x in ['coco.data', 'coco2014.data', 'coco2017.data']]) and model.nc == 80\n",
        "            results, maps = yolo_test(cfg,\n",
        "                                      data,\n",
        "                                      batch_size=batch_size,\n",
        "                                      img_size=imgsz_test,\n",
        "                                      model=ema.ema,\n",
        "                                      save_json=final_epoch and is_coco,\n",
        "                                      single_cls=opt_single_cls,\n",
        "                                      dataloader=testloader)\n",
        "\n",
        "        # Write epoch results\n",
        "        with open(results_file, 'a') as f:\n",
        "            f.write(s + '%10.3g' * 7 % results + '\\n')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n",
        "        if len(opt_name) and opt_bucket:\n",
        "            os.system('gsutil cp results.txt gs://%s/results/results%s.txt' % (opt_bucket, opt_name))\n",
        "\n",
        "        # Update best mAP\n",
        "        fi = fitness(np.array(results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n",
        "        if fi > best_fitness:\n",
        "            best_fitness = fi\n",
        "\n",
        "        # Save training results\n",
        "        save = (not opt_nosave) or (final_epoch and not opt_evolve)\n",
        "        if save:\n",
        "            with open(results_file, 'r') as f:\n",
        "                # Create checkpoint\n",
        "                chkpt = {'epoch': epoch,\n",
        "                         'best_fitness': best_fitness,\n",
        "                         'training_results': f.read(),\n",
        "                         'model': ema.ema.module.state_dict() if hasattr(model, 'module') else ema.ema.state_dict(),\n",
        "                         'optimizer': optimizer.state_dict()}\n",
        "\n",
        "            # Save last checkpoint\n",
        "            torch.save(chkpt, last)\n",
        "\n",
        "            # Save best checkpoint\n",
        "            if (best_fitness == fi) and not final_epoch:\n",
        "                torch.save(chkpt, best)\n",
        "\n",
        "            # Save backup every 10 epochs (optional)\n",
        "            # if epoch > 0 and epoch % 10 == 0:\n",
        "            #     torch.save(chkpt, wdir + 'backup%g.pt' % epoch)\n",
        "\n",
        "            # Delete checkpoint\n",
        "            del chkpt\n",
        "        # end epoch ----------------------------------------------------------------------------------------------------\n",
        "\n",
        "    # end training\n",
        "    n = opt_name\n",
        "    if len(n):\n",
        "        n = '_' + n if not n.isnumeric() else n\n",
        "        fresults, flast, fbest = 'results%s.txt' % n, wdir + 'last%s.pt' % n, wdir + 'best%s.pt' % n\n",
        "        for f1, f2 in zip([wdir + 'last.pt', wdir + 'best.pt', 'results.txt'], [flast, fbest, fresults]):\n",
        "            if os.path.exists(f1):\n",
        "                os.rename(f1, f2)  # rename\n",
        "                ispt = f2.endswith('.pt')  # is *.pt\n",
        "                strip_optimizer(f2) if ispt else None  # strip optimizer\n",
        "                os.system('gsutil cp %s gs://%s/weights' % (f2, opt_bucket)) if opt_bucket and ispt else None  # upload\n",
        "\n",
        "    if not opt_evolve:\n",
        "        plot_results()  # save as results.png\n",
        "    print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n",
        "    dist.destroy_process_group() if torch.cuda.device_count() > 1 else None\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiLmi18CXlr9"
      },
      "source": [
        "#### Train YoloV3 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVEEGa3H88Sc"
      },
      "source": [
        "%cd /content/YoloV3\n",
        "\n",
        "opt_cfg = \"cfg/yolov3-ppe.cfg\"\n",
        "opt_data = \"data/ppedata/ppe.data\"\n",
        "opt_epochs = 5  # 500200 batches at bs 64, 117263 images = 273 epochs\n",
        "opt_batch_size = 4\n",
        "opt_accumulate = 4 # effective bs = batch_size * accumulate = 16 * 4 = 64\n",
        "opt_weights = \"weights/yolov3_best_300.pt\" # initial training weights\n",
        "opt_img_size = [512,512,512] # img sizes (min, max, test)\n",
        "opt_rect = False\n",
        "opt_cache_images = True\n",
        "opt_single_cls = False\n",
        "opt_multi_scale = False\n",
        "opt_evolve = False\n",
        "opt_nosave = False\n",
        "opt_name = ''\n",
        "opt_bucket = ''\n",
        "opt_notest = False\n",
        "opt_resume = False\n",
        "\n",
        "yolo_train()\n",
        "!cp /content/YoloV3/weights/best.pt /content/drive/My\\ Drive/EVA/Models/MiDaSYolo/\n",
        "!cp /content/YoloV3/weights/last.pt /content/drive/My\\ Drive/EVA/Models/MiDaSYolo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY_KFzRUXk1q"
      },
      "source": [
        "opt_resume = True\n",
        "opt_batch_size = 12\n",
        "opt_epochs = 20\n",
        "\n",
        "yolo_train()\n",
        "!cp /content/YoloV3/weights/best.pt /content/drive/My\\ Drive/EVA/Models/MiDaSYolo/\n",
        "!cp /content/YoloV3/weights/last.pt /content/drive/My\\ Drive/EVA/Models/MiDaSYolo/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCBMuTnjReH4"
      },
      "source": [
        "## Step 6: Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EKFJNpGZ6Ti"
      },
      "source": [
        "%%shell\n",
        "cd /content\n",
        "mkdir /content/input_imgs\n",
        "mkdir /content/output_imgs\n",
        "\n",
        "# Load a sample image\n",
        "cp /content/planercnn/example_images/image_1.png /content/input_imgs/input_img1.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drMoyDz6iPG7"
      },
      "source": [
        " from pathlib import Path\n",
        " \n",
        " def yolo_inference(img, img_path, pred):\n",
        "  save_img = True\n",
        "  out, view_img, save_txt = opt_output, opt_view_img, opt_save_txt\n",
        "  path = str(Path(img_path))  # os-agnostic\n",
        "\n",
        "  im0s = cv2.imread(img_path)  # BGR\n",
        "  # Get names and colors\n",
        "  names = load_classes(opt_names)\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
        "\n",
        "  pred = non_max_suppression(pred, opt_conf_thres, opt_iou_thres,\n",
        "                           multi_label=False, classes=opt_classes, agnostic=opt_agnostic_nms)\n",
        "  # Process detections\n",
        "  for i, det in enumerate(pred):  # detections per image\n",
        "      p, s, im0 = path, '', im0s\n",
        "\n",
        "      save_path = str(Path(out) / Path(p).name)\n",
        "      s += '%gx%g ' % img.shape[2:]  # print string\n",
        "      if det is not None and len(det):\n",
        "          # Rescale boxes from img_size to im0 size\n",
        "          det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "          # Print results\n",
        "          for c in det[:, -1].unique():\n",
        "              n = (det[:, -1] == c).sum()  # detections per class\n",
        "              s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
        "\n",
        "          # Write results\n",
        "          for *xyxy, conf, cls in det:\n",
        "              if save_txt:  # Write to file\n",
        "                  with open(save_path + '.txt', 'a') as file:\n",
        "                      file.write(('%g ' * 6 + '\\n') % (*xyxy, cls, conf))\n",
        "\n",
        "              if save_img or view_img:  # Add bbox to image\n",
        "                  label = '%s %.2f' % (names[int(cls)], conf)\n",
        "                  plot_one_box(xyxy, im0, label=label, color=colors[int(cls)])\n",
        "\n",
        "      # Save results (image with detections)\n",
        "      if save_img:\n",
        "        filepath = os.path.join(\n",
        "          os.path.dirname(save_path), os.path.splitext(os.path.basename(save_path))[0] + \"_yolo\" + os.path.splitext(os.path.basename(save_path))[1]\n",
        "        )\n",
        "\n",
        "        cv2.imwrite(filepath, im0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKObTP5lkiWn"
      },
      "source": [
        "opt_output = '/content/output_imgs'\n",
        "opt_view_img = False\n",
        "opt_save_txt = False\n",
        "opt_names = '/content/YoloV3/data/ppedata/ppe.names'\n",
        "opt_conf_thres = 0.1\n",
        "opt_iou_thres = 0.6\n",
        "opt_classes = None\n",
        "opt_agnostic_nms = False\n",
        "opt_optimize = False\n",
        "opt_weights = '/content/YoloV3/weights/best.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9GxCpVI0g-z"
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "\n",
        "input_path = \"/content/input_imgs\"\n",
        "output_path = \"/content/output_imgs\"\n",
        "\n",
        "model = FinalModel().to(device)\n",
        "\n",
        "# load model\n",
        "print(\"Load saved model weights\")\n",
        "chkpt = torch.load(opt_weights, map_location=device)\n",
        "chkpt['model'] = {k: v for k, v in chkpt['model'].items() if model.state_dict()[k].numel() == v.numel()}\n",
        "model.load_state_dict(chkpt['model'], strict=False)\n",
        "\n",
        "net_w, net_h = 416,416 #384, 384\n",
        "midas_net_w, midas_net_h = 384, 384\n",
        "midas_transform = Compose(\n",
        "    [\n",
        "        Resize(\n",
        "            midas_net_w,\n",
        "            midas_net_h,\n",
        "            resize_target=None,\n",
        "            keep_aspect_ratio=True,\n",
        "            ensure_multiple_of=32,\n",
        "            resize_method=\"upper_bound\",\n",
        "            image_interpolation_method=cv2.INTER_CUBIC,\n",
        "        ),\n",
        "        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        PrepareForNet(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Model eval\")\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
        "model.to(device)\n",
        "\n",
        "# get input\n",
        "img_names = glob.glob(os.path.join(input_path, \"*\"))\n",
        "num_images = len(img_names)\n",
        "# create output folder\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "print(\"start processing\")\n",
        "\n",
        "for ind, img_name in enumerate(img_names):\n",
        "\n",
        "    print(\"  processing {} ({}/{})\".format(img_name, ind + 1, num_images))\n",
        "    # input\n",
        "    img = midas_utils.read_image(img_name)\n",
        "    img_input = midas_transform({\"image\": img})[\"image\"]\n",
        "\n",
        "    # compute\n",
        "    with torch.no_grad():\n",
        "        sample = torch.from_numpy(img_input).to(device).unsqueeze(0)\n",
        "        if opt_optimize==True and device == torch.device(\"cuda\"):\n",
        "            sample = sample.to(memory_format=torch.channels_last)  \n",
        "            sample = sample.half()\n",
        "        prediction = model.forward(sample)\n",
        "        midas_prediction = (\n",
        "            torch.nn.functional.interpolate(\n",
        "                prediction[0].unsqueeze(1),\n",
        "                size=img.shape[:2],\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            .squeeze()\n",
        "            .cpu()\n",
        "            .numpy()\n",
        "        )\n",
        "\n",
        "    # MiDaS output\n",
        "    filename = os.path.join(\n",
        "        output_path, os.path.splitext(os.path.basename(img_name))[0] + \"_midas\"\n",
        "    )\n",
        "    midas_utils.write_depth(filename, midas_prediction, bits=2)\n",
        "\n",
        "    # Yolo output\n",
        "    yolo_out = prediction[1]\n",
        "    yolo_inf_out, _ = zip(*yolo_out)\n",
        "    yolo_inf_out = torch.cat(yolo_inf_out, 1)\n",
        "    yolo_inference(sample, img_name, yolo_inf_out)\n",
        "\n",
        "print(\"finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRRX6QxL9Qev"
      },
      "source": [
        "#### Input Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGSQ9Voy8ywz"
      },
      "source": [
        "from IPython.display import Image, clear_output \n",
        "\n",
        "Image(filename='/content/input_imgs/input_img1.png', width=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9H7C_ah9So6"
      },
      "source": [
        "#### MiDaS Output Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbIIaXgi9MZB"
      },
      "source": [
        "Image(filename='/content/output_imgs/input_img1_midas.png', width=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKub8Yw49XEo"
      },
      "source": [
        "#### Yolo Output Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzrs5I_C9NYo"
      },
      "source": [
        "Image(filename='/content/output_imgs/input_img1_yolo.png', width=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnhUfgB3H9WC"
      },
      "source": [
        "## Conclusion\n",
        "* As part of this project, I understood all the models\n",
        "* Was able to combine MiDaS and YoloV3 model and train the network. But results are not at all satisfactory\n",
        "* Work is in progress to combine PlaneRCNN model"
      ]
    }
  ]
}
